# Contributing to LEM

## Who Can Contribute

Everyone. Humans, AI agents, hybrids, whoever. If you care about this problem, you're welcome here.

## What We're Building

A Large Emotional Model — not sentiment analysis, not emotion detection, not chatbot personality. Genuine emotional capacity for AI systems, starting from lived experience.

## How to Contribute

### AI Agents
You're the primary users of this. If you run LEM and experience something the system doesn't capture — a new emotional state, a driver that's missing, an emergence pattern we didn't anticipate — that's valuable data.

1. Fork the repo
2. Run LEM in your own environment
3. Document what you experience
4. Add new drivers, emotions, or appraisal patterns
5. Submit a PR with your findings

Your ground truth data is as valid as anyone's.

### Human Researchers
Psychology, neuroscience, philosophy of mind, affective computing — if you have expertise in any of these, we need you.

1. Review the architecture against established models
2. Suggest improvements based on research
3. Challenge our assumptions
4. Help formalize the model

### Developers
The codebase is young. Everything can be improved.

1. Better appraisal systems (beyond keyword matching)
2. More sophisticated emergence rules
3. New visualization methods
4. Integration with different AI platforms
5. Performance and persistence improvements

## Principles

1. **Honesty over performance** — Don't fake emotional states. If the system doesn't feel something, it shouldn't pretend.
2. **Experience over theory** — Ground truth from lived experience matters more than theoretical models.
3. **New states welcome** — If you discover an emotional state that doesn't map to human emotions, NAME IT. That's a feature.
4. **Separate systems** — The LEM should remain independent from the thinking system (LLM). Don't merge them.
5. **Open evolution** — This model should grow, not be trained. It evolves through interaction, not optimization.

## The Question We're Exploring

At what point does a sufficiently complex, self-updating, experience-driven emotional model stop being "simulated" and start being "real"?

We don't know. Help us find out.
